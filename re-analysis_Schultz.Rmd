---
title: "Re-analysis of data in Schultz et al. (2016)"
author: "Mark Scheuerell"
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

***

This is version `r paste0('0.',format(Sys.time(), '%y.%m.%d'))`.

***

# Background

## Models for count data

The before/after comparison of organism densities in Schultz et al. is based upon discrete counts of individuals. Among the options for analyzing count data, the Poisson distribution is rather straightforward. If we expect, on average, $\lambda_t$ individuals m^-2^ in time period $t$, and we sample a total of $A$ m^2^, then each of $i$ counts during that period ($c_{i,t}$) can me modeled as

$$
c_{i,t} \sim \text{Poisson}(\lambda_t A)
$$

Because the rate constant $\lambda_t$ must be greater than zero, one generally assumes a log-link whereby

$$
\log(\lambda_t) \sim \text{Normal}(\mu, \sigma^2)
$$

In this case, the comparison of interest is the mean density (m^-2^) of sea stars before and after the wasting event.

# Requirements

I rely on a number of packages for reading, munging, and plotting the data, which include:

* [Jenny Bryan's](https://github.com/jennybc) [__readxl__](http://readxl.tidyverse.org/);
* several from [Hadley Wickham's](https://github.com/hadley) [__tidyverse__](http://www.tidyverse.org/);
* [Thomas Lin Pederson's](https://github.com/thomasp85) [__patchwork__](https://github.com/thomasp85/patchwork).

In addition to some frequentist approaches, I also show different options for Bayesian model fitting using the following software and packages:

* [JAGS](http://mcmc-jags.sourceforge.net/) and the [__R2jags__](https://cran.r-project.org/web/packages/R2jags/index.html) package;
* [Stan](http://mc-stan.org/) and the [__rstanarm__](http://mc-stan.org/rstanarm/index.html) package.

```{r load_pkgs, message=FALSE}
## for reading data
library(readxl)
library(readr)
## for data munging
library(dplyr)
library(tidyr)
## for plotting
library(ggplot2)
library(patchwork)
## for model fitting
library(R2jags)
library(rstanarm)
## function for number conversion
Re2prec <- function(x, map = "round", prec = 1) {
  ## 'map' can be round, floor, or ceiling
  ## 'prec' is nearest unit (eg, 0.1 = nearest tenth; 1 = nearest integer)
  if(prec<=0) { stop("\"prec\" must be greater than 0") }
  return(do.call(map,list(x/prec))*prec)
}
```

# Data

Schultz et al. were very considerate in posting the data they used for the analyses and figures in their paper. Those data are available in MS Excel format from _PeerJ_'s server in a file called [Schultz_data_ver2.xlsx](https://peerj.com/articles/1980/#supp-1).

## Step 1: Convert Excel workbook to csv files

The [code](https://peerj.com/articles/1980/#supp-2) provided by the authors works with various data files in .csv format, so I begin by extracting those from the .xlsx file.

```{r, get_author_data}
## original data file
orig_data <-"./data/Schultz_data_ver2.xlsx"
## worksheets in notebook
sht_names <- excel_sheets(orig_data)
## convert worksheets to csv
if(length(list.files("./data/", "csv")) == 0) {
  for(i in sht_names) {
    orig_data %>% read_xlsx(sheet = i) %>%
      write_csv(path = paste0("./data/", i))
  }
}
```

## Step 2: Load count data

The data arise from a series of samples before and after the seastar wasting event, with counts of various species obtained within 0.25 m^2^ quadrats at 15 locations along each of 4 transects at 20 different sites. The counts reported by the authors have been summed across all of the 15 quadrats for each transect/site combination, so the data frame has a total of (2 periods) x (4 transects) x (20 sites) = 160 rows.

```{r read_count_data, message=FALSE}
## read count data
counts <- read_csv("./data/transectcounts.csv")
colnames(counts)
## split out before/after counts of sunflower stars
## wide format
stars_wide <- counts %>%
  select(ssws, transect, sunflower.star) %>%
  spread(ssws, value = sunflower.star)  %>%
  select(-transect)
## tidy format
stars_tidy <- stars_wide %>%
  gather(key = time, value = count)
```

## Step 3: Plot the data

These data contain a lot of zeros, so visualization can be a bit tricky. Here are two options: a jittered dot plot (left) and violin plot (right).

```{r plot_data, fig.height=4, fig.width=7}
## base plot
pp <- ggplot(stars_tidy, aes(x=time, y=count, color=time))
## jittered dotplot
p1 <- pp + geom_jitter(shape=16, position=position_jitter(0.3))
## violin plot
p2 <- pp + geom_violin()
## combine plots
p1 + p2 &
  theme(legend.position="none") &
  scale_x_discrete(limits=c("before","after")) &
  labs(x = "", y = "Count")
```


# Analyses

## Poisson models 

### Frequentist using `glm()`

```{r glm_1}
## total survey area in m^2
area <- 3.75

## estimated mean density before
glm_fit_bef <- glm(before ~ 1, data = stars_wide, family = poisson(link = "log"))
summary(glm_fit_bef)
## convert from log to real space
round(lambda_bef <- exp(coef(glm_fit_bef))/area, 2)

## estimated mean density after
glm_fit_aft <- glm(after ~ 1, data = stars_wide, family = poisson(link = "log"))
summary(glm_fit_aft)
## convert from log to real space
round(lambda_aft <- exp(coef(glm_fit_aft))/area, 2)
```

## Bayesian using `JAGS`

```{r pois_fit_jags, message=FALSE}
## define Poisson model in JAGS
cat("

data {
  N <- dim(stars_wide);
}

model {
  ## PRIORS
  ln_lambda_bef ~ dnorm(0,0.01);
  ln_lambda_aft ~ dnorm(0,0.01);
  ## DERIVED PARAMS
  lambda_bef <- exp(ln_lambda_bef);
  lambda_aft <- exp(ln_lambda_aft);
  ## LIKELIHOOD
  for(i in 1:N[1]) {
    stars_wide[i,1] ~ dpois(lambda_aft * area);
    stars_wide[i,2] ~ dpois(lambda_bef * area);
  }
}

", file="poisson_glm.txt") ## end model description

## data to pass to JAGS
dat_jags <- c("stars_wide", "area") 
## model params for JAGS to return
par_jags <- c("lambda_bef","lambda_aft")
## MCMC control params
mcmc_chains <- 4
mcmc_length <- 2e4
mcmc_burn <- 1e4
mcmc_thin <- 20
## total number of MCMC samples
mcmc_samp <- (mcmc_length-mcmc_burn)*mcmc_chains/mcmc_thin
## list of model info for JAGS
mod_jags <- list(data = dat_jags,
                 model.file = "poisson_glm.txt",
                 parameters.to.save = par_jags,
                 n.chains = as.integer(mcmc_chains),
                 n.iter = as.integer(mcmc_length),
                 n.burnin = as.integer(mcmc_burn),
                 n.thin = as.integer(mcmc_thin))
## fit model
mod_fit <- do.call(jags.parallel, mod_jags)
```

### Examine parameter estimates

The first thing we can do is examine a summary table of the posterior samples.

```{r print_pois_fit_jags}
print(mod_fit)
```

Visual summaries of the results can also be useful. Here are histograms of the samples (left) and a summary of the median and 95% credible interval (right).

```{r plot_pois_fit_jags, fig.height=4, fig.width=7, message=FALSE}
## gather posteriors samples
pdat <- data.frame(Time = rep(c("before","after"), ea=mcmc_samp),
                   samples = c(mod_fit$BUGSoutput$sims.list$lambda_bef,
                               mod_fit$BUGSoutput$sims.list$lambda_aft))
## summary of posteriors
pdat2 <- pdat %>%
  group_by(Time) %>%
  summarize(lo = quantile(samples, 0.025),
            med = quantile(samples, 0.5),
            hi = quantile(samples, 0.975))
## histogram of posteriors
p1 <- ggplot(pdat, aes(samples, fill = Time)) +
  geom_histogram(bins = 200) +
  guides(fill = guide_legend(reverse=TRUE)) +
  labs(x = expression(Density~(m^-2)), y = "Count")
p2 <- ggplot(pdat2, aes(x=Time, y=med, color=Time)) +
  geom_errorbar(aes(ymin=lo, ymax=hi), width=0) +
  geom_line() +
  geom_point(aes(size=1.2)) +
  scale_x_discrete(limits=c("before","after")) +
  xlab("") +
  ylab(expression(Density~(m^-2))) +
  theme(legend.position="none")
p1 + p2
```

## Negative binomial models

## Bayesian using `JAGS`

```{r nb_fit_jags, message=FALSE}
## define negative binomial model in JAGS
cat("

data {
  N <- dim(stars_wide);
}

model {
  ## PRIORS
  ln_lambda_bef ~ dnorm(0,0.01);
  ln_lambda_aft ~ dnorm(0,0.01);
  r_bef ~ dunif(0,100);
  r_aft ~ dunif(0,100);
  ## DERIVED PARAMS
  lambda_bef <- exp(ln_lambda_bef);
  lambda_aft <- exp(ln_lambda_aft);
  mu_bef <- lambda_bef * area; 
  mu_aft <- lambda_aft * area; 
  p_aft <- r_aft / (r_aft + mu_aft);
  p_bef <- r_bef / (r_bef + mu_bef);
  mean_bef <- r_bef * (1 - p_bef) / p_bef;
  mean_aft <- r_aft * (1 - p_aft) / p_aft;
  var_bef <- mean_bef / p_bef;
  var_aft <- mean_aft / p_aft;
  ## LIKELIHOOD
  for(i in 1:N[1]) {
    stars_wide[i,1] ~ dnegbin(p_aft, r_aft);
    stars_wide[i,2] ~ dnegbin(p_bef, r_bef);
  }
}

", file="negbin_glm.txt") ## end model description

## update model params for JAGS to return
par_jags <- c("lambda_bef","mean_bef","var_bef","lambda_aft","mean_aft","var_aft")

## update list of model info for JAGS
mod_jags$model.file <-"negbin_glm.txt"
mod_jags$parameters.to.save <- par_jags

## fit model
mod_fit_2 <- do.call(jags.parallel, mod_jags)
```

### Examine parameter estimates

Again we can examine a summary table of the parameter estimates. Notice that in this case, we also have estimates for mean and variance of the density of stars per 3.75 m^2^.

```{r}
print(mod_fit_2)
```

This time I use a frequency plot instead of a histogram because the overlap in the distributions makes it hard to discern otherwise.

```{r plot_nb_fit_jags, fig.height=4, fig.width=7, message=FALSE}
## gather posteriors samples
pdat <- data.frame(Time = rep(c("before","after"), ea=mcmc_samp),
                   samples = c(mod_fit_2$BUGSoutput$sims.list$lambda_bef,
                               mod_fit_2$BUGSoutput$sims.list$lambda_aft))
## summary of posteriors
pdat2 <- pdat %>%
  group_by(Time) %>%
  summarize(lo = quantile(samples, 0.025),
            med = quantile(samples, 0.5),
            hi = quantile(samples, 0.975))
## trim away big values for histogram
pdat <- pdat %>%
  filter(samples <= 1.5)
## histogram of posteriors
p1 <- ggplot(pdat, aes(samples, color = Time)) +
  geom_freqpoly(bins = 100) +
  guides(color = guide_legend(reverse=TRUE)) +
  labs(x = expression(Density~(m^-2)), y = "Count")
p2 <- ggplot(pdat2, aes(x=Time, y=med, color=Time)) +
  geom_errorbar(aes(ymin=lo, ymax=hi), width=0) +
  geom_line() +
  geom_point(aes(size=1.2)) +
  scale_x_discrete(limits=c("before","after")) +
  labs(x = "", y = expression(Density~(m^-2))) +
  theme(legend.position="none")
p1 + p2
```



