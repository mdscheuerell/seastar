---
title: "Re-analysis of data in Schultz et al. (2016)"
author: "Mark Scheuerell"
date: "1/24/2018"
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

***

This is version `r paste0('0.',format(Sys.time(), '%y.%m.%d'))`.

***

# Background

## Model for count data

The Poisson distribution is a rather straightforward option for modeling count data. If we expect, on average, $\lambda_t$ individuals m^-1^ in each of the 2 time periods $t$, and we sample a total of $A$ m^2^, then each of $i$ counts from time period $t$ ($c_{i,t}$) can me modeled as

$$
c_{i,t} \sim \text{Poisson}(\lambda_t A)
$$

Because the rate constant $\lambda_t$ must be greater than zero, one generally assumes a log-link whereby

$$
\log(\lambda_t) \sim \text{Normal}(\mu, \sigma^2)
$$

In this case, the comparison of interest is the mean density (m^-1^) of sea stars before and after the wasting event.

# Data

Schultz et al. were very considerate in posting the data they used for the analyses and figures in their paper. Those data are available in MS Excel format from _PeerJ_'s server in a file called [Schultz_data_ver2.xlsx](https://peerj.com/articles/1980/#supp-1). To read the data into __R__, I'll use [Jenny Bryan](https://github.com/jennybc)'s [__readxl__](http://readxl.tidyverse.org/) package.

## Step 1: Convert raw Excel workbook to csv files

```{r, get_author_data}
library(readxl)
library(tidyverse)
## original data file
orig_data <-"./data/Schultz_data_ver2.xlsx"
## Worksheets in notebook
sht_names <- excel_sheets(orig_data)
## Covert worksheets to csv
if(length(list.files("./data/", "csv")) == 0) {
  for(i in sht_names) {
    tmp_csv <- read_xlsx(orig_data, sheet = i)
    write_csv(tmp_csv, path = paste0("./data/", i))
  }
}
```

## Step 2: Load count data

The data arise from a series of samples before and after the seastar wasting event, with counts of various species obtained within 0.25 m^2^ quadrats at 15 locations along each of 4 transects at 20 different sites. The counts reported by the authors have been summed across all of the 15 quadrats for each transect/site combination, so the data frame has a total of (2 periods) x (4 transects) x (20 sites) = 160 rows.

```{r read_count_data}
## get count data
counts <- read_csv("./data/transectcounts.csv")
colnames(counts)
## split out before/after counts of sunflower stars
stars <- counts %>%
  select(ssws, transect, sunflower.star) %>%
  spread(ssws, value = sunflower.star)  %>%
  select(-transect)
```

# Analyses

## Poisson GLM 

### Frequentist using `glm`

```{r glm_1}
## total survey area in m^2
area <- 3.75

## estimated mean density before
b_fit <- glm(before ~ 1, data = stars, family = poisson(link = "log"))
summary(b_fit)
b_mean <- exp(sum(coef(b_fit)))/area
b_mean

## estimated mean density after
a_fit <- glm(after ~ 1, data = stars, family = poisson(link = "log"))
summary(a_fit)
a_mean <- exp(sum(coef(a_fit)))/area
a_mean
```

## Bayesian using `JAGS`

```{r pois_glm_jags, message=FALSE}
library(R2jags)

cat("

data {
  N <- dim(stars);
}

model {
  ## PRIORS
  ln_lambda_bef ~ dnorm(0,0.01);
  ln_lambda_aft ~ dnorm(0,0.01);
  lambda_bef <- exp(ln_lambda_bef);
  lambda_aft <- exp(ln_lambda_aft);
  ## LIKELIHOOD
  for(i in 1:N[1]) {
    stars[i,1] ~ dpois(lambda_aft * area);
    stars[i,2] ~ dpois(lambda_bef * area);
  }
} ## end model description

", file="poisson_glm.txt")


## 1. data to pass to JAGS
dat_jags <- c("stars", "area") 

## 2. model params for JAGS to return
par_jags <- c("lambda_bef","lambda_aft")

## 3. MCMC control params
mcmc_chains <- 4
mcmc_length <- 2e4
mcmc_burn <- 1e4
mcmc_thin <- 20
## total number of MCMC samples
mcmc_samp <- (mcmc_length-mcmc_burn)*mcmc_chains/mcmc_thin

## list of model info for JAGS
mod_jags <- list(data = dat_jags,
                 model.file = "poisson_glm.txt",
                 parameters.to.save = par_jags,
                 n.chains = as.integer(mcmc_chains),
                 n.iter = as.integer(mcmc_length),
                 n.burnin = as.integer(mcmc_burn),
                 n.thin = as.integer(mcmc_thin))

## fit model
mod_fit <- do.call(jags.parallel, mod_jags)
print(mod_fit)
tbl_smry <- mod_fit$BUGSoutput$summary[par_jags, c("mean","sd","2.5%","50%","97.5%")]
print(tbl_smry, digits = 2, quote = FALSE, justify = "right")
```

### Examine fits

```{r plot_fits, fig.height=6, fig.width=4}
Re2prec <- function(x, map = "round", prec = 1) {
  if(prec<=0) { stop("\"prec\" must be greater than 0") }
  do.call(map,list(x/prec))*prec
}
## plot posteriors
par(mfrow=c(2,1), mai=c(0.8,0.8,0.2,0.2), omi=c(0,0,0,0))
xmx <- Re2prec(max(mod_fit$BUGSoutput$sims.list$lambda_aft,
                   mod_fit$BUGSoutput$sims.list$lambda_bef), "ceiling", 0.02)
brk <- seq(0, xmx, 0.01)
hist(mod_fit$BUGSoutput$sims.list$lambda_bef, breaks = brk,
     col="blue", border=NA,
     main="", xlab=expression(lambda[before]))
hist(mod_fit$BUGSoutput$sims.list$lambda_aft, breaks = brk,
     col="blue", border=NA,
     main="", xlab=expression(lambda[after]))

```

## Negative binomial GLM

## Bayesian using `JAGS`

```{r nb_glm_jags, message=FALSE}
cat("

data {
  N <- dim(stars);
}

model {
  ## PRIORS
  ln_lambda_bef ~ dnorm(0,0.01);
  ln_lambda_aft ~ dnorm(0,0.01);
  r_bef ~ dunif(0,100);
  r_aft ~ dunif(0,100);
  ## DERIVED PARAMS
  lambda_bef <- exp(ln_lambda_bef);
  lambda_aft <- exp(ln_lambda_aft);
  mu_bef <- lambda_bef * area; 
  mu_aft <- lambda_aft * area; 
  p_aft <- r_aft / (r_aft + mu_aft);
  p_bef <- r_bef / (r_bef + mu_bef);
  mean_bef <- r_bef * (1 - p_bef) / p_bef;
  mean_aft <- r_aft * (1 - p_aft) / p_aft;
  var_bef <- mean_bef / p_bef;
  var_aft <- mean_aft / p_aft;
  ## LIKELIHOOD
  for(i in 1:N[1]) {
    stars[i,1] ~ dnegbin(p_aft, r_aft);
    stars[i,2] ~ dnegbin(p_bef, r_bef);
  }
} ## end model description

", file="negbin_glm.txt")


## update model params for JAGS to return
par_jags <- c("lambda_bef","mean_bef","var_bef","lambda_aft","mean_aft","var_aft")

## update list of model info for JAGS
mod_jags$model.file <-"negbin_glm.txt"
mod_jags$parameters.to.save <- par_jags

## fit model
mod_fit_2 <- do.call(jags.parallel, mod_jags)
print(mod_fit_2)
tbl_smry <- mod_fit_2$BUGSoutput$summary[par_jags, c("mean","sd","2.5%","50%","97.5%")]
print(tbl_smry, digits = 2, quote = FALSE, justify = "right")
```

```{r, plot_rates, fig.height=4, fig.width=4}
pdat <- tbl_smry[c(1,4), c("2.5%","50%","97.5%")]
par(mai=c(0.5,0.9,0.1,0.1), omi=c(0,0,0,0))
plot(c(1,2), pdat[,"50%"], pch=16, cex=1.5,
     xlim=c(0.5,2.5), ylim=c(0,max(pdat)),
     xlab="", xaxt="n", ylab=expression(Density~(m^-2)))
segments(c(1,2), pdat[,"2.5%"], c(1,2), pdat[,"97.5%"], lwd=2)
axis(1, at = c(1,2), labels = c("Before","After"), tick = FALSE)
```


### Examine fits

In this case, the extra parameter for overdispersion leads to very long right tails of the distributions for the mean density. Thus, for plotting purposes only, we'll trim the largest 1% of values.

```{r plot_fits_nb, fig.height=6, fig.width=4}
plot_aft <- sort(mod_fit_2$BUGSoutput$sims.list$lambda_aft)[1:(mcmc_samp - mcmc_samp/100)]
## plot posteriors
par(mfrow=c(2,1), mai=c(0.8,0.8,0.2,0.2), omi=c(0,0,0,0))
xmx <- Re2prec(max(plot_aft, mod_fit_2$BUGSoutput$sims.list$lambda_bef), "ceiling", 0.05)
brk <- seq(0, xmx, 0.05)
hist(mod_fit_2$BUGSoutput$sims.list$lambda_bef, breaks = brk,
     col="blue", border=NA,
     main="", xlab=expression(lambda[before]))
hist(plot_aft, breaks = brk,
     col="blue", border=NA,
     main="", xlab=expression(lambda[after]))
```

